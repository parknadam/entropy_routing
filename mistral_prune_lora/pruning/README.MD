# Mistral Layer Pruning

Mistral 7B ëª¨ë¸ì„ ìœ„í•œ ë ˆì´ì–´ í”„ë£¨ë‹ êµ¬í˜„ì…ë‹ˆë‹¤. LLaMA-2 í”„ë£¨ë‹ ì½”ë“œë¥¼ Mistralì— ë§ê²Œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

ë…¼ë¬¸: ["The Unreasonable Ineffectiveness of the Deeper Layers" (ICLR 2025)](https://arxiv.org/abs/2403.17887)

## ğŸ”‘ í•µì‹¬ ì°¨ì´ì : Mistral vs LLaMA-2

### ì•„í‚¤í…ì²˜
- **LLaMA-2**: Standard Multi-Head Attention (MHA)
- **Mistral**: Grouped-Query Attention (GQA) + Sliding Window Attention
- **Layer êµ¬ì¡°**: ë‘˜ ë‹¤ `model.model.layers` ì‚¬ìš© (ë™ì¼!)

### Hyperparameters (ë…¼ë¬¸ Table 1)
| Parameter | LLaMA-2 7B | Mistral 7B |
|-----------|------------|------------|
| Learning Rate | 3e-4 | **3e-6** (100ë°° ë‚®ìŒ) |
| LoRA Rank | 2 | **4** (2ë°° ë†’ìŒ) |
| LoRA Alpha | 2 | 4 |
| FFN Modules | gate_proj, down_proj, up_proj | ë™ì¼ |
| Num Layers | 32 | 32 |
| Hidden Size | 4096 | 4096 |

### Pruning ê²°ê³¼ (ë…¼ë¬¸)
- **Mistral 7B**: 30-40% ë ˆì´ì–´ ì œê±° ê°€ëŠ¥ (minimal MMLU degradation)
- **LLaMA-2 7B**: 25-35% ë ˆì´ì–´ ì œê±° ê°€ëŠ¥
- **ê³µí†µì **: ê¹Šì€ ë ˆì´ì–´ì¼ìˆ˜ë¡ ìœ ì‚¬ë„ ë†’ìŒ, ë§ˆì§€ë§‰ ë ˆì´ì–´ëŠ” í•„ìˆ˜

---

## ğŸ“¦ ì„¤ì¹˜

```bash
pip install torch transformers datasets safetensors peft accelerate
```

---

## ğŸš€ ì‚¬ìš©ë²•

### 1. Layer Pruning (A ëª¨ë¸ + B/C ë²ˆë“¤ ìƒì„±)

```bash
# Mistral 7B Pruning
python -m pruning.layeronly_drop \
  --model mistralai/Mistral-7B-v0.1 \
  --device cuda:0 \
  --drop_frac 0.25 \
  --keep_last_layer \
  --nsamples 64 \
  --seqlen 1024 \
  --max_batches 32 \
  --save_dir ./mistral_results/pruning/A \
  --save_removed_dir ./mistral_results/pruning/bundles
```

**ì¶œë ¥ êµ¬ì¡°:**
```
mistral_results/
â”œâ”€â”€ pruning/
â”‚   â”œâ”€â”€ A/                          # í”„ë£¨ë‹ëœ ëª¨ë¸ (24 layers)
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”‚   â”œâ”€â”€ manifest.json           # í”„ë£¨ë‹ ì •ë³´
â”‚   â”‚   â”œâ”€â”€ layers_map.json         # ë ˆì´ì–´-í‚¤ ë§¤í•‘
â”‚   â”‚   â””â”€â”€ original_config/        # ì›ë³¸ ì„¤ì •
â”‚   â””â”€â”€ bundles/
â”‚       â”œâ”€â”€ B/                      # ì œê±°ëœ ë ˆì´ì–´ ê·¸ë£¹ 1
â”‚       â”‚   â”œâ”€â”€ layer_021.safetensors
â”‚       â”‚   â”œâ”€â”€ layer_022.safetensors
â”‚       â”‚   â””â”€â”€ bundle_meta.json
â”‚       â””â”€â”€ C/                      # ì œê±°ëœ ë ˆì´ì–´ ê·¸ë£¹ 2
â”‚           â”œâ”€â”€ layer_025.safetensors
â”‚           â”œâ”€â”€ layer_026.safetensors
â”‚           â””â”€â”€ bundle_meta.json
```

### 2. Dynamic Stage Routing (A/AB/FULL)

```bash
# Entropy ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…
python -m pruning.entropy_cli \
  --base_model ./mistral_results/pruning/A \
  --bundles_dir ./mistral_results/pruning/bundles \
  --max_new_tokens 128 \
  --device cuda:0
```

**ì‘ë™ ë°©ì‹:**
1. **Stage A**: Pruned modelë§Œ (24 layers)
2. **Stage AB**: A + Bundle B ë³µì› (28 layers)
3. **Stage FULL**: A + B + C ëª¨ë‘ ë³µì› (32 layers)

í”„ë¡¬í”„íŠ¸ ë³µì¡ë„ì— ë”°ë¼ ìë™ìœ¼ë¡œ stage ì„ íƒ:
- ê°„ë‹¨í•œ ì§ˆë¬¸ â†’ A (ë¹ ë¦„)
- ì¤‘ê°„ ë³µì¡ë„ â†’ AB
- ë³µì¡í•œ ì§ˆë¬¸ â†’ FULL (ì •í™•í•¨)

---

## ğŸ¯ ëª¨ë¸ ìë™ ê°ì§€

ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ëŠ” ëª¨ë¸ íƒ€ì…ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤:

```python
# ì§€ì› ëª¨ë¸
- mistralai/Mistral-7B-v0.1
- mistralai/Mistral-7B-Instruct-v0.2
- meta-llama/Llama-2-7b-hf
- meta-llama/Llama-2-7b-chat-hf
- meta-llama/Llama-2-13b-hf
- facebook/opt-6.7b
```

---

## ğŸ“Š Pruning ì „ëµ

### Angular Distance ê¸°ë°˜ ì„ íƒ
```python
d(â„“) = (1/Ï€) * arccos(mean_cos(x(â„“), x(â„“+n)))
```

- ê°€ì¥ ë‚®ì€ `d(â„“)` ì°¾ê¸° â†’ ê°€ì¥ ìœ ì‚¬í•œ ì—°ì† ë¸”ë¡ ì œê±°
- `keep_last_layer=True`: ë§ˆì§€ë§‰ ë ˆì´ì–´ í•­ìƒ ìœ ì§€ (í•„ìˆ˜!)

### B/C Split ì •ì±…
```bash
--split_policy half       # 50:50 ë¶„í•  (ê¸°ë³¸ê°’)
--split_policy ratio      # ì»¤ìŠ¤í…€ ë¹„ìœ¨
--split_ratio 0.3         # 30:70 ë¶„í• 
```

---

## ğŸ”§ ì£¼ìš” íŒŒì¼ ì„¤ëª…

| íŒŒì¼ | ì—­í•  | Mistral ìˆ˜ì • |
|------|------|-------------|
| `identity.py` | PassLayer ì •ì˜ | MistralDecoderLayer ì§€ì› ì¶”ê°€ |
| `simdrop.py` | Angular distance ê³„ì‚° | ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€ |
| `layeronly_drop.py` | ë©”ì¸ í”„ë£¨ë‹ ìŠ¤í¬ë¦½íŠ¸ | Mistral config ì²˜ë¦¬ |
| `bundler.py` | B/C ë²ˆë“¤ ì €ì¥ | model_type íŒŒë¼ë¯¸í„° ì¶”ê°€ |
| `entropy_cli.py` | ë™ì  ë¼ìš°íŒ… REPL | Mistral/LLaMA í†µí•© |
| `data.py` | C4 ë°ì´í„° ë¡œë” | ìˆ˜ì • ë¶ˆí•„ìš” (ê³µí†µ) |

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# 1. Pruning ì‹¤í–‰ (drop_frac=0)
python -m pruning.layeronly_drop \
  --model mistralai/Mistral-7B-v0.1 \
  --drop_frac 0.0 \
  --save_dir ./test/A

# 2. ëª¨ë¸ íƒ€ì… í™•ì¸
python -c "
from transformers import AutoConfig
cfg = AutoConfig.from_pretrained('./test/A')
print(f'Model type: {cfg.model_type}')
print(f'Num layers: {cfg.num_hidden_layers}')
"

# 3. Entropy CLI í…ŒìŠ¤íŠ¸
python -m pruning.entropy_cli \
  --base_model ./test/A \
  --bundles_dir ./test/bundles \
  --max_new_tokens 32
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ (ë…¼ë¬¸ ê¸°ì¤€)

| Model | Original | Pruned (30%) | MMLU Î” |
|-------|----------|--------------|--------|
| Mistral 7B | 32 layers | 22 layers | -2.1% |
| LLaMA-2 7B | 32 layers | 24 layers | -3.4% |
| LLaMA-2 13B | 40 layers | 28 layers | -1.8% |

**Mistralì˜ ì¥ì :**
- ê°™ì€ í”„ë£¨ë‹ ë¹„ìœ¨ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ ë” ì ìŒ
- GQA ë•ë¶„ì— inference ì†ë„ë„ ë” ë¹ ë¦„

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. Learning Rate (ì¤‘ìš”!)
```bash
# LoRA í•™ìŠµ ì‹œ ë°˜ë“œì‹œ Mistralìš© LR ì‚¬ìš©
--learning_rate 3e-6  # Mistral (NOT 3e-4!)
--lora_r 4            # Mistral (NOT 2!)
```

### 2. Memory ìš”êµ¬ì‚¬í•­
- **Pruning**: ~20GB (1x A100/3090)
- **Stage routing**: ~12GB (pruned model)
- **Full model**: ~16GB (all bundles loaded)

### 3. Batch Size
```bash
# Mistralì€ LLaMAë³´ë‹¤ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
--nsamples 64      # OK for Mistral
--max_batches 32   # ì¶©ë¶„í•¨
```

---

## ğŸ”— ì°¸ê³  ìë£Œ

- ë…¼ë¬¸: https://arxiv.org/abs/2403.17887
- Mistral: https://mistral.ai/
- HuggingFace: https://huggingface.co/mistralai

---

## ğŸ“ Citation

```bibtex
@article{gromov2024unreasonable,
  title={The Unreasonable Ineffectiveness of the Deeper Layers},
  author={Gromov, Andrey and Tirumala, Kushal and Shapourian, Hassan and Glorioso, Paolo and Roberts, Daniel A},
  journal={ICLR},
  year={2025}
}
```

---

## ğŸ†š LLaMA vs Mistral ì½”ë“œ í˜¸í™˜ì„±

ê¸°ì¡´ LLaMA ì½”ë“œì™€ **100% í˜¸í™˜**ë©ë‹ˆë‹¤:

```bash
# LLaMA-2 (ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ë™ì‘)
python -m pruning.layeronly_drop \
  --model meta-llama/Llama-2-7b-hf \
  ...

# Mistral (ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
python -m pruning.layeronly_drop \
  --model mistralai/Mistral-7B-v0.1 \
  ...
```

**ìë™ ê°ì§€ ë¡œì§:**
1. `config.model_type` í™•ì¸
2. `config.architectures` í™•ì¸
3. `model.model.layers` vs `model.model.decoder.layers` êµ¬ì¡° í™•ì¸

---

## ğŸ› Troubleshooting

### Q: "Cannot import MistralDecoderLayer" ì—ëŸ¬
A: transformers ì—…ë°ì´íŠ¸
```bash
pip install --upgrade transformers>=4.34.0
```

### Q: Bundle restore ì‹œ shape mismatch
A: manifest.jsonì—ì„œ model_type í™•ì¸
```bash
cat ./mistral_results/pruning/A/manifest.json | grep model_type
```

### Q: OOM (Out of Memory)
A: 
```bash
--max_batches 16    # ì¤„ì´ê¸°
--seqlen 512        # ì¤„ì´ê¸°
--nsamples 32       # ì¤„ì´ê¸°
```

---

**Happy Pruning! ğŸ‰**